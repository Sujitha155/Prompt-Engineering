# -*- coding: utf-8 -*-
"""Prompt Engineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TKf91J_FBGITvcW1Et88ENNbJL13hbJB
"""

!pip install datasets

from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
from datasets import load_metric
import numpy as np

model_name = "distilbert-base-uncased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)

prompts = {
    "Simple": "Answer the question based on the text:\nText: {}\nQuestion: {}",
    "Instructional": "Carefully read the text below and provide a precise answer to the question:\nText: {}\nQuestion: {}",
    "Detailed": "You are an expert in the field. Read the following text and answer the question thoroughly:\nText: {}\nQuestion: {}",
    "Structured": "Provide a well-structured answer to the question based on the text:\n1. Identify the main idea.\n2. Consider any key details.\nText: {}\nQuestion: {}",
    "Conversational": "Imagine you are explaining the answer to a friend. Read the text and then answer the question:\nText: {}\nQuestion: {}"
}

# Get user input for the text (context)
texts = input("Please enter the text (context): ")

# Get user input for the question
questions = input("Please enter the question: ")

# Get user input for the reference answer (correct answer)
reference_answer = input("Please enter the correct answer (reference): ")

def generate_answers(prompt, texts, questions):
    answers = []
    for text, question in zip(texts, questions):
        full_prompt = prompt.format(text, question)
        result = qa_pipeline(question=question, context=text)
        answers.append(result['answer'])
    return answers

def evaluate_answers(answers, references):
    metric = load_metric("accuracy")
    scores = []
    for answer, reference in zip(answers, references):
        scores.append(answer == reference)
    accuracy = np.mean(scores)
    return accuracy

results = {}

# Iterate through each prompt design in the prompts dictionary
for name, prompt in prompts.items():
    print(f"Evaluating prompt: {name}")

    # Generate answers for the current prompt design
    generated_answers = generate_answers(prompt, texts, questions)

    reference_answers=reference_answer

     # Evaluate the generated answers against the reference answers
    accuracy = evaluate_answers(generated_answers, reference_answers)

    # Store the accuracy result
    results[name] = accuracy

for name, accuracy in results.items():
    print(f"\nPrompt: {name}")
    print(f"Accuracy: {accuracy:.2f}")